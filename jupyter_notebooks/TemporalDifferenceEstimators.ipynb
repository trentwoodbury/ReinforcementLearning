{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "We have the below Markov Decision Process:<br>\n",
    "<img src='images/MDP.png'>\n",
    "<br>\n",
    "For this MDP, we want to find a lambda value such that the Expected Value of this MDP is the same as the expected value, were we to use TD(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's program the k-step estimators for k=1 through k=6 (this will be the same as k=infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_step_estimators(prob_state_1, value_estimates, rewards):\n",
    "    '''\n",
    "    This outputs the first 5 k-estimators for the given input.\n",
    "    INPUT \n",
    "        prob_state_1: probability of transfering to state 1 (versus to state 2)\n",
    "        value_estimates: list of numbers. Each number represents the value estimate\n",
    "            for the state of the same index.\n",
    "        rewards: list of numbers. Each number represents the reward associated with \n",
    "            the state of the same index.\n",
    "    OUTPUT\n",
    "        k_est_values: list of floats. Each float represents the k-estimator value\n",
    "            associated with the input for k = list index\n",
    "    '''\n",
    "    k_est_values = []\n",
    "    path_1 = [0,1,3,4,5,6]\n",
    "    path_2 = [0,2,3,4,5,6]\n",
    "    \n",
    "    rewards_1_indices = [0,2,4,5,6]\n",
    "    rewards_2_indices = [1,3,4,5,6]\n",
    "\n",
    "    rewards_1 = np.array(rewards)[rewards_1_indices]\n",
    "    rewards_2 = np.array(rewards)[rewards_2_indices]\n",
    "    \n",
    "    val_ests_1 = np.array(value_estimates)[path_1]\n",
    "    val_ests_2 = np.array(value_estimates)[path_2]\n",
    "    \n",
    "    for k in range(1,6):\n",
    "        path_1 = prob_state_1 * (\n",
    "            value_estimates[0] + np.sum(rewards_1[:k]) + val_ests_1[k]\n",
    "        )\n",
    "        path_2 = (1 - prob_state_1) * (\n",
    "            value_estimates[0] + np.sum(rewards_2[:k]) + val_ests_2[k]\n",
    "        )\n",
    "        k_est_values.append(path_1 + path_2)\n",
    "        \n",
    "    return k_est_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_td_1(prob_state_1, value_estimates, rewards):\n",
    "    '''\n",
    "    Computes TD(1) for the above MDP, given the inputs. \n",
    "    INPUT \n",
    "    prob_state_1: probability of transfering to state 1 (versus to state 2)\n",
    "    rewards: list of numbers. Each number represents the reward associated with \n",
    "        the state of the same index.\n",
    "    OUTPUT \n",
    "        td_1: float.\n",
    "    '''\n",
    "    rewards_1_indices = [0,2,4,5,6]\n",
    "    rewards_2_indices = [1,3,4,5,6]\n",
    "\n",
    "    rewards_1 = np.array(rewards)[rewards_1_indices]\n",
    "    rewards_2 = np.array(rewards)[rewards_2_indices]\n",
    "    \n",
    "    td_1 = (\n",
    "        (prob_state_1 * np.sum(rewards_1)) + \n",
    "        ((1 - prob_state_1) * np.sum(rewards_2))\n",
    "    )\n",
    "        \n",
    "    return td_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run through an example to ensure that this function is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.5, 4.0, 4.0, 6.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "k_est_values = k_step_estimators(\n",
    "    prob_state_1=0.5, \n",
    "    value_estimates=[0, 3, 8, 2, 1, 2, 0], \n",
    "    rewards=[0, 0, 0, 4, 1, 1, 1]\n",
    ")\n",
    "\n",
    "print k_est_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "td_1 = get_td_1(\n",
    "    prob_state_1=0.5, \n",
    "    value_estimates=[0, 3, 8, 2, 1, 2, 0], \n",
    "    rewards=[0, 0, 0, 4, 1, 1, 1]\n",
    ")\n",
    "print td_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when performing TD($\\lambda$), We know that our Expected Value of this MDP will be:<br>\n",
    "$(1 - \\lambda) E_{1} \\, + \\, \\lambda(1-\\lambda)E_{2} \\, + \\,\n",
    "\\lambda^{2}(1-\\lambda)E_{3} \\, + \\, \\lambda^{3}(1-\\lambda)E_{4} \\, + \\,\n",
    "\\lambda^{4}(1-\\lambda)E_{5}\n",
    "\\, + \\, [(1-\\lambda) \\cdot TD(1) \\cdot \\sum_{i=6}^{\\infty}{\\lambda^{i-1}}]\n",
    "$\n",
    "<br><br>\n",
    "However, because we have (tacitly) assumed that $\\gamma=1$ up to this point, we need a terminating state. Obviously, T=5 acts as our terminating state. So we can rewrite this as:\n",
    "\n",
    "$(1 - \\lambda) E_{1} \\, + \\, \\lambda(1-\\lambda)E_{2} \\, + \\,\n",
    "\\lambda^{2}(1-\\lambda)E_{3} \\, + \\, \\lambda^{3}(1-\\lambda)E_{4} \\, + \\,\n",
    "\\lambda^{4}(1-\\lambda)E_{5} \\, + \\, \\lambda^{4}(1-\\lambda)TD(1) = TD(1)$ \n",
    "\n",
    "Doing some algebra, we can rewrite this as a polynomial with respect to $\\lambda$:\n",
    "<br>\n",
    "$\n",
    "(E_{1} - TD(1)) \\, + \\,\n",
    "\\lambda(E_{2} - E_{1}) \\, +\\, \n",
    "\\lambda^{2}(E_{3} - E_{2}) \\, +\\, \n",
    "\\lambda^{3}(E_{4} - E_{3}) \\, +\\, \n",
    "\\lambda^{4}(E_{5} - E_{4}) \\, +\\, \n",
    "\\lambda^{5}(TD(1) - E_{5}) \\, = 0\n",
    "$\n",
    "\n",
    "Now, we just need to solve this polynomial for $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_lambda(k_ests, td_1):\n",
    "    '''\n",
    "    Finds the lambda that gives us the same expected value as TD(1), according to the \n",
    "    formulas above.\n",
    "    INPUT \n",
    "        k_ests: list of floats. Output of k_step_estimators.\n",
    "        td_1: Our Expected value given TD(1)\n",
    "    OUTPUT \n",
    "        lam: Float. Solution lambda value.\n",
    "    '''\n",
    "    coef_1 = td_1 - k_ests[4] # lambda ** 5 term\n",
    "    coef_2 = k_ests[4] - k_ests[3] # lambda ** 4 term\n",
    "    coef_3 = k_ests[3] - k_ests[2] # lambda ** 3 term\n",
    "    coef_4 = k_ests[2] - k_ests[1] # lambda ** 2 term\n",
    "    coef_5 = k_ests[1] - k_ests[0] # lambda term\n",
    "    coef_6 = k_ests[0] - td_1 # constant\n",
    "    coeffs = [coef_1, coef_2, coef_3, coef_4, coef_5, coef_6]\n",
    "    \n",
    "    lam = np.roots(coeffs)\n",
    "    return lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.85463768  1.45160596  1.          0.40303172]\n"
     ]
    }
   ],
   "source": [
    "print find_lambda(k_est_values, td_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify, by hand, that .40303 will result in the same Expected Value.\n",
    "\n",
    "Now that we've verified this is working for a simple case, let's build a pipeline to take in \n",
    "our original inputs and output our $\\lambda$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expectation_pipeline(prob_state_1, value_estimates, rewards):\n",
    "    '''\n",
    "    Runs a full pipeline to produce a set of lambda solutions given the input.\n",
    "    INPUT \n",
    "    prob_state_1: probability of transfering to state 1 (versus to state 2)\n",
    "    value_estimates: list of numbers. Each number represents the value estimate\n",
    "        for the state of the same index.\n",
    "    rewards: list of numbers. Each number represents the reward associated with \n",
    "        the state of the same index.\n",
    "    OUTPUT \n",
    "        lam: Float. Solution lambda value\n",
    "    '''\n",
    "    k_est_vals = k_step_estimators(prob_state_1, value_estimates, rewards)\n",
    "    td_1 = get_td_1(prob_state_1, value_estimates, rewards)\n",
    "    lam = find_lambda(k_est_vals, td_1)\n",
    "    return lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.80023857 -0.91864104  1.          0.45006241]\n"
     ]
    }
   ],
   "source": [
    "print expectation_pipeline(0.31, [0.0,22.1,20.4,8.6,-4.1,0.0,0.0], [7.0,3.7,5.2,-0.3,-4.4,7.7,9.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
