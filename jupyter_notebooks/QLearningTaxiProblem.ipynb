{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> What is this notebook? </b><br>\n",
    "This notebook includes an implementation of Q-Learning and the Taxi problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def take_epsilon_greedy_action(epsilon, policy, state):\n",
    "    '''\n",
    "    returns action based on epsilon greedy policy\n",
    "    '''\n",
    "    choices = range(6)\n",
    "    eps_greedy = np.random.choice(np.arange(0,1.01,.001))\n",
    "    if eps_greedy > epsilon:\n",
    "        action = policy[state]\n",
    "    else:\n",
    "        action = np.random.choice(choices)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_decay(epsilon, iter_count):\n",
    "    '''\n",
    "    returns decayed epsilon\n",
    "    '''\n",
    "    return epsilon * np.exp(-.001 * iter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_q_learning(\n",
    "        taxi_env, \n",
    "        epsilon=1, \n",
    "        alpha=.5, \n",
    "        gamma=0.9, \n",
    "        decay=False, \n",
    "        episode_count=1000\n",
    "):\n",
    "    '''\n",
    "    Performs Q-Learning on taxi environment by exploring aggressively until\n",
    "    it learns the optimal policy. This applies a decaying epsilon greedy method.\n",
    "    '''\n",
    "    # Matrix of values for state action pairs\n",
    "    Q = np.zeros((5000, 6))\n",
    "    choices = range(6)\n",
    "    policy = np.array([np.random.choice(choices) for _ in range(Q.shape[0])])\n",
    "    \n",
    "    # Q-learning loop, we run through 10000 episodes\n",
    "    for iter_count in range(episode_count):\n",
    "        if iter_count % 1000 == 0 and iter_count > 0:\n",
    "            print \"Iteration {}\".format(iter_count)\n",
    "        \n",
    "        step_count = 0\n",
    "        taxi_env.reset()\n",
    "        # status is (state, reward, Done, prob.)\n",
    "        status = (0,0,False,{'prob':1})\n",
    "        \n",
    "        while True:\n",
    "            state = status[0]\n",
    "            # \n",
    "            if decay:\n",
    "                decayed_eps = epsilon_decay(epsilon, iter_count)\n",
    "            else:\n",
    "                decayed_eps = epsilon\n",
    "                \n",
    "            action = take_epsilon_greedy_action(decayed_eps, policy, state)\n",
    "\n",
    "            status = taxi_env.step(action)\n",
    "            prob = status[-1]['prob']\n",
    "            step_count += 1\n",
    "            reward = status[1]\n",
    "            next_state = status[0]\n",
    "            if next_state == state:\n",
    "                continue\n",
    "            next_action = policy[next_state]\n",
    "            \n",
    "            # terminal state case\n",
    "            if status[2]:\n",
    "                Q[state, action] += alpha * (reward - Q[state, action])\n",
    "                break\n",
    "                \n",
    "            # Normal Q-learning update\n",
    "            Q[state, action] += (\n",
    "                alpha * (reward + \n",
    "                (gamma * Q[next_state, next_action]) - Q[state, action])\n",
    "            )\n",
    "            \n",
    "            # policy updates\n",
    "            Q_state = list(Q[state])\n",
    "            policy[state] = Q_state.index(max(Q_state))\n",
    "            \n",
    "            if step_count >= 200:\n",
    "                break\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mae(list_1, list_2):\n",
    "    '''\n",
    "    Produces Mean Absolute Error given two iterables.\n",
    "    '''\n",
    "    mae = np.average([np.abs(x - y) for x,y in zip(list_1, list_2)])\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000\n",
      "Iteration 2000\n",
      "Iteration 3000\n",
      "Iteration 4000\n",
      "Iteration 5000\n",
      "Iteration 6000\n",
      "Iteration 7000\n",
      "Iteration 8000\n",
      "Iteration 9000\n",
      "MAE : 8.12841079731\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "test_indices = [(462, 4), (398, 3), (253, 0), (377, 1), (83, 5)]\n",
    "test_values = [-11.374402515, 4.348907, -0.5856821173, 9.683, -12.8232660372]\n",
    "\n",
    "Q = perform_q_learning(\n",
    "    env, \n",
    "    epsilon=1, \n",
    "    alpha=.65, \n",
    "    decay=True, \n",
    "    episode_count=10000\n",
    ")\n",
    "Q_estimates = [Q[index] for index in test_indices]\n",
    "mae = get_mae(Q_estimates, test_values)\n",
    "print \"MAE :\", mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = (12, 5)\n",
    "# plt.plot(range(len(updates)), updates)\n",
    "# plt.title(\"Average Learning by Episode\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Average Learning\")\n",
    "# plt.xticks([])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
