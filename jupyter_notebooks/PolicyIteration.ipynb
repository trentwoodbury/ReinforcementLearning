{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is this notebook?</b><br>\n",
    "This notebook includes code for the policy iteration algorithm. This algorithm takes as input the Markov Decision process (shown below) and returns the number of iterations required for the algorithm to terminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's design an MDP that takes a median of 20 iterations for policy iteration to converge\n",
    "mdp = {\"gamma\": 0.75, \"states\": []}\n",
    "total_len = 29\n",
    "\n",
    "state_0 =  {\"id\": 0, \"actions\": [{\"transitions\": [{\"to\": 0, \"reward\": 0, \"id\": 0, \"probability\": 1}], \"id\": 0}, {\"transitions\": [{\"to\": 1, \"reward\": -2, \"id\": 0, \"probability\": 1}], \"id\": 1}]}\n",
    "penultimate_state = {\"id\": total_len - 1, \"actions\": [{\"transitions\": [{\"to\": total_len - 2, \"reward\": -1, \"id\": 0, \"probability\": 1}], \"id\": 0}, {\"transitions\": [{\"to\": total_len, \"reward\": 5 * total_len, \"id\": 0, \"probability\": 1}], \"id\": 1}]}\n",
    "final_state = {\"id\": total_len, \"actions\": [{\"transitions\": [{\"to\": total_len - 1, \"reward\": -1, \"id\": 0, \"probability\": 1}], \"id\": 0}, {\"transitions\": [{\"to\": total_len, \"reward\": 0, \"id\": 0, \"probability\": 1}], \"id\": 1}]}\n",
    "\n",
    "# Fill in states\n",
    "mdp[\"states\"].append(state_0)\n",
    "for i in range(1, total_len -1):\n",
    "    if i % 8 == 0:\n",
    "        current_state = {\n",
    "          \"id\": i,\n",
    "          \"actions\": [\n",
    "            {\n",
    "              \"id\": 0,\n",
    "              \"transitions\": [\n",
    "                {\n",
    "                  \"id\": 0,\n",
    "                  \"probability\": 1,\n",
    "                  \"reward\": -1,\n",
    "                  \"to\": i - 1\n",
    "                }  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"id\": 1,\n",
    "              \"transitions\": [\n",
    "                {\n",
    "                  \"id\": 0,\n",
    "                  \"probability\": 1,\n",
    "                  \"reward\": -3,\n",
    "                  \"to\": i + 1\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "    else:\n",
    "                current_state = {\n",
    "          \"id\": i,\n",
    "          \"actions\": [\n",
    "            {\n",
    "              \"id\": 0,\n",
    "              \"transitions\": [\n",
    "                {\n",
    "                  \"id\": 0,\n",
    "                  \"probability\": 1,\n",
    "                  \"reward\": -1,\n",
    "                  \"to\": i - 1\n",
    "                }  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"id\": 1,\n",
    "              \"transitions\": [\n",
    "                {\n",
    "                  \"id\": 0,\n",
    "                  \"probability\": 1,\n",
    "                  \"reward\": -1,\n",
    "                  \"to\": i + 1\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "    mdp[\"states\"].append(current_state)\n",
    "mdp[\"states\"].append(penultimate_state)\n",
    "mdp[\"states\"].append(final_state)\n",
    "\n",
    "with open('/Users/twoodbury/gatech/ReinforcementLearning/PolicyIteration/json_files/mdp_7.json', 'wb') as f:\n",
    "    json.dump(mdp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_value(V, action, gamma):\n",
    "    '''\n",
    "    Calculates value of given action.\n",
    "    INPUT \n",
    "        V: numpy vector. Vector of values.\n",
    "        action: action item in MDP dictionary. Includes 'transitions' key.\n",
    "        gamma: float. Discount factor.\n",
    "    OUTPUT\n",
    "        Value: float.\n",
    "    '''\n",
    "    Value = np.sum([\n",
    "        transition['probability'] * transition['reward'] + (gamma * V[transition['to']])\n",
    "        for transition in action['transitions']\n",
    "    ])\n",
    "    return Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def perform_policy_evaluation(V, max_delta, theta, states, current_policy, gamma):\n",
    "    '''\n",
    "    Performs evaluation step to product Value vector.\n",
    "    INPUT \n",
    "        V: vector of length num_states. \n",
    "        max_delta: float. biggest delta value.\n",
    "        theta: float. Cutoff value. Once max_delta < theta, we cut off this evaluation.\n",
    "        states: dictionary. Item in MDP dict.\n",
    "        current_policy: dictionary. Created in policy iteration function.\n",
    "    '''\n",
    "    # Policy Evaluation\n",
    "    while max_delta > theta:\n",
    "        max_delta = 0\n",
    "        for idx, state in enumerate(states):\n",
    "            # get value of each state\n",
    "            v = V[idx]\n",
    "            V[idx] = get_value(V, current_policy[idx], gamma)\n",
    "            delta = np.abs(v - V[idx])\n",
    "\n",
    "            if delta > max_delta:\n",
    "                max_delta = delta\n",
    "                \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def policy_improvement(states, current_policy, V, gamma):\n",
    "    '''\n",
    "    Performs policy improvement component of policy iteration.\n",
    "    INPUT \n",
    "        states: dictionary. MDP item.\n",
    "        current_policy: dictionary. Created in policy iteration function.\n",
    "        V: vector of state-action values.\n",
    "        gamma: float. \n",
    "    OUTPUT\n",
    "        policy_stable: boolean. Whether we've \"converged\" or not.\n",
    "        current_policy: dictionary. Updated (or not) policy.\n",
    "    '''\n",
    "    policy_stable = True\n",
    "    for idx, state in enumerate(states):\n",
    "        b = current_policy[idx]\n",
    "        policy_value = V[idx]\n",
    "        for action in state['actions']:\n",
    "            action_value =  get_value(V, action, gamma)\n",
    "            if action_value > policy_value:\n",
    "                current_policy[idx] = action\n",
    "        if current_policy[idx] != b:\n",
    "            policy_stable = False\n",
    "            \n",
    "    return policy_stable, current_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_initial_policy(states):\n",
    "    '''\n",
    "    Builds intial policy based on random action for each state.\n",
    "    '''\n",
    "    initial_policy = []\n",
    "    for state in states:\n",
    "        possible_actions = state['actions']\n",
    "        action = np.random.choice(possible_actions)\n",
    "        initial_policy.append(action)\n",
    "    \n",
    "    return initial_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, theta=.0000000001):\n",
    "    '''\n",
    "    Performs policy iteration on MDP and returns number of iterations required for algorithm\n",
    "    to terminate.\n",
    "    INPUT \n",
    "        mdp: dictionary, see cell above for example.\n",
    "        theta: minimum update threshold between iterations.\n",
    "    OUTPUT:\n",
    "        num_iterations: integer. Number of PI iterations required, given MDP.\n",
    "    '''\n",
    "    gamma = mdp['gamma']\n",
    "    states = mdp['states']\n",
    "    \n",
    "    # Initialize all states' Q values to 0.\n",
    "    num_states = len(states)\n",
    "    Q = np.zeros(num_states)\n",
    "    V = np.zeros(num_states)\n",
    "    #TODO: build initial policy that's assured to be coherent\n",
    "    \n",
    "    current_policy = build_initial_policy(states)\n",
    "    policy_outcomes = []\n",
    "    max_delta = 1\n",
    "    \n",
    "    V = perform_policy_evaluation(V, max_delta, theta, states, current_policy, gamma)\n",
    "    \n",
    "    # Policy Improvement\n",
    "    num_iterations = 0\n",
    "    policy_stable = False\n",
    "    while policy_stable == False:\n",
    "        policy_stable, current_policy = policy_improvement(states, current_policy, V, gamma)\n",
    "        num_iterations += 1\n",
    "    \n",
    "    return num_iterations, current_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " [{'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': 0, 'to': 0}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 0}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 1}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 2}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 5}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 4}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 7}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 6}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 7}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 10}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 11}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 12}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 13}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 12}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 15}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 14}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 15}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 18}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 19}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 20}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 21}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 22}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 21}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 22}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 23}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 26}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 25}]},\n",
       "  {'id': 0,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': -1, 'to': 26}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': 145, 'to': 29}]},\n",
       "  {'id': 1,\n",
       "   'transitions': [{'id': 0, 'probability': 1, 'reward': 0, 'to': 29}]}])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: catch bug here causing early termination\n",
    "policy_iteration(mdp, theta=.0000000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
